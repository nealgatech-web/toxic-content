{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hate Speech Detection using Traditional Machine Learning Approaches\n",
    "\n",
    "This notebook implements classical ML models: Logistic Regression, Random Forest, and LightGBM."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Configuration and Constants\n",
    "RANDOM_SEED = 1234\n",
    "TEST_SIZE = 0.2\n",
    "VAL_SIZE = 0.25\n",
    "DATA_PATH = '/content/drive/MyDrive/hatespeech/hatexplain_detailed.csv'\n",
    "MAX_ITER = 1000\n",
    "N_BOOTSTRAP_ITERATIONS = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Standard library imports\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Third-party imports - Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Third-party imports - Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Third-party imports - NLP\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer, SnowballStemmer\n",
    "\n",
    "# Third-party imports - Machine Learning\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    f1_score, accuracy_score, classification_report, \n",
    "    confusion_matrix, roc_curve, auc\n",
    ")\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Third-party imports - Other ML libraries\n",
    "from lightgbm import LGBMClassifier\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from scipy.sparse import hstack\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "# Google Colab setup\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive/', force_remount=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Data Loading and Initial Processing\n",
    "def load_and_prepare_data(file_path, random_seed=RANDOM_SEED):\n",
    "    \"\"\"Load data and perform initial transformations.\"\"\"\n",
    "    # Load data\n",
    "    raw_data = pd.read_csv(file_path, index_col=0)\n",
    "    \n",
    "    # Reset index and select columns\n",
    "    processed_data = raw_data.reset_index()\n",
    "    processed_data = processed_data[['text', 'text_type']]\n",
    "    \n",
    "    # Convert labels to binary (0 for normal, 1 for toxic)\n",
    "    label_mapping = {'normal': 0}\n",
    "    processed_data['text_type'] = processed_data['text_type'].map(\n",
    "        lambda x: 0 if x == 'normal' else 1\n",
    "    )\n",
    "    \n",
    "    # Shuffle data\n",
    "    processed_data = processed_data.sample(\n",
    "        frac=1, random_state=random_seed\n",
    "    ).reset_index(drop=True)\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "# Execute data loading\n",
    "dataframe_main = load_and_prepare_data(DATA_PATH)\n",
    "\n",
    "# Display basic info\n",
    "print(f\"Dataset shape: {dataframe_main.shape}\")\n",
    "print(f\"\\nDataset info:\")\n",
    "print(dataframe_main.describe().T)\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(dataframe_main.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Text Preprocessing Pipeline\n",
    "class TextPreprocessor:\n",
    "    \"\"\"Handles all text preprocessing operations.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Clean text by removing URLs, mentions, hashtags, and special chars.\"\"\"\n",
    "        text = str(text).lower()\n",
    "        text = re.sub(pattern=r\"http\\S+\", repl=\"<URL>\", string=text)\n",
    "        text = re.sub(pattern=r\"@\\w+\", repl=\"<USER>\", string=text)\n",
    "        text = re.sub(pattern=r\"#\\w+\", repl=\"<HASHTAG>\", string=text)\n",
    "        text = re.sub(pattern=r\"[^\\w\\s]\", repl=\"\", string=text)\n",
    "        text = re.sub(pattern=r\"\\s+\", repl=\" \", string=text)\n",
    "        return text.strip()\n",
    "    \n",
    "    def remove_stopwords(self, text):\n",
    "        \"\"\"Remove stopwords from text.\"\"\"\n",
    "        words = text.split()\n",
    "        filtered_words = filter(lambda w: w not in self.stop_words, words)\n",
    "        return ' '.join(filtered_words)\n",
    "    \n",
    "    def lemmatize(self, text):\n",
    "        \"\"\"Lemmatize words in text.\"\"\"\n",
    "        words = text.split()\n",
    "        lemmatized_words = map(self.lemmatizer.lemmatize, words)\n",
    "        return ' '.join(lemmatized_words)\n",
    "    \n",
    "    def preprocess(self, texts):\n",
    "        \"\"\"Apply full preprocessing pipeline.\"\"\"\n",
    "        # Step 1: Clean\n",
    "        cleaned = list(map(self.clean_text, texts))\n",
    "        # Step 2: Remove stopwords\n",
    "        no_stopwords = list(map(self.remove_stopwords, cleaned))\n",
    "        # Step 3: Lemmatize\n",
    "        lemmatized = list(map(self.lemmatize, no_stopwords))\n",
    "        return lemmatized\n",
    "\n",
    "# Initialize preprocessor and process data\n",
    "preprocessor = TextPreprocessor()\n",
    "dataframe_classical_ml = dataframe_main.copy()\n",
    "dataframe_classical_ml['text'] = preprocessor.preprocess(dataframe_classical_ml['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Data Splitting Function\n",
    "def split_data(features, labels, test_size=TEST_SIZE, val_size=VAL_SIZE, random_seed=RANDOM_SEED):\n",
    "    \"\"\"Split data into train, validation, and test sets.\"\"\"\n",
    "    # First split: train+val vs test\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        features, labels, test_size=test_size, random_state=random_seed\n",
    "    )\n",
    "    \n",
    "    # Second split: train vs val\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp, y_temp, test_size=val_size, random_state=random_seed\n",
    "    )\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "# Split the data\n",
    "features = dataframe_classical_ml['text'].tolist()\n",
    "labels = dataframe_classical_ml['text_type'].tolist()\n",
    "\n",
    "train_features_classical, val_features_classical, test_features_classical, \\\n",
    "train_labels_classical, val_labels_classical, test_labels_classical = split_data(\n",
    "    features, labels\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_features_classical)}\")\n",
    "print(f\"Validation samples: {len(val_features_classical)}\")\n",
    "print(f\"Test samples: {len(test_features_classical)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TF-IDF Vectorization\n",
    "import joblib\n",
    "\n",
    "# Load pre-trained vectorizer\n",
    "try:\n",
    "    tfidf_vectorizer = joblib.load(\"tfidf_vectorizer_2_binary.pkl\")\n",
    "    print(\"Loaded pre-trained TF-IDF vectorizer\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Vectorizer file not found, would need to train one\")\n",
    "    tfidf_vectorizer = None\n",
    "\n",
    "if tfidf_vectorizer is not None:\n",
    "    # Transform all splits\n",
    "    train_features_classical = tfidf_vectorizer.transform(train_features_classical)\n",
    "    val_features_classical = tfidf_vectorizer.transform(val_features_classical)\n",
    "    test_features_classical = tfidf_vectorizer.transform(test_features_classical)\n",
    "    print(\"TF-IDF transformation complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Class Distribution Visualization\n",
    "status_counts = dataframe_classical_ml['text_type'].value_counts()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "status_counts.plot(kind='bar', ax=ax)\n",
    "\n",
    "# Add value labels on bars\n",
    "for idx in range(len(status_counts)):\n",
    "    v = status_counts.iloc[idx]\n",
    "    ax.text(idx, v, str(v), ha='center', va='bottom')\n",
    "\n",
    "ax.set_title('Label Distribution by text_type')\n",
    "ax.set_xlabel('text_type')\n",
    "ax.set_ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nClass distribution shows imbalance - will use class weights during training\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Model Development and Performance Analysis\n",
    "\n",
    "## 3.1 Traditional Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Model Training Utilities\n",
    "class ModelTrainer:\n",
    "    \"\"\"Helper class for training and evaluating models.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def train_and_evaluate(model, X_train, y_train, X_val, y_val):\n",
    "        \"\"\"Train model and return validation F1 score.\"\"\"\n",
    "        model.fit(X_train, y_train)\n",
    "        predictions = model.predict(X_val)\n",
    "        f1 = f1_score(y_val, predictions, average='weighted')\n",
    "        return f1, model\n",
    "    \n",
    "    @staticmethod\n",
    "    def grid_search(param_grid, model_class, X_train, y_train, X_val, y_val, **model_kwargs):\n",
    "        \"\"\"Perform grid search using nested loops.\"\"\"\n",
    "        best_score = 0\n",
    "        best_model = None\n",
    "        best_params = None\n",
    "        \n",
    "        # Get parameter names and values\n",
    "        param_names = list(param_grid.keys())\n",
    "        param_values = [param_grid[name] for name in param_names]\n",
    "        \n",
    "        # Nested loop iteration\n",
    "        def nested_loop_search(depth, current_params):\n",
    "            nonlocal best_score, best_model, best_params\n",
    "            \n",
    "            if depth == len(param_names):\n",
    "                # All parameters set, train and evaluate\n",
    "                try:\n",
    "                    model = model_class(**current_params, **model_kwargs)\n",
    "                    score, trained_model = ModelTrainer.train_and_evaluate(\n",
    "                        model, X_train, y_train, X_val, y_val\n",
    "                    )\n",
    "                    \n",
    "                    param_str = ', '.join([f\"{k}={v}\" for k, v in current_params.items()])\n",
    "                    print(f\"Validation F1-Score for {param_str}: {score:.4f}\")\n",
    "                    \n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_model = trained_model\n",
    "                        best_params = current_params.copy()\n",
    "                except Exception as e:\n",
    "                    param_str = ', '.join([f\"{k}={v}\" for k, v in current_params.items()])\n",
    "                    print(f\"Skipping {param_str} due to error: {e}\")\n",
    "            else:\n",
    "                # Recursively set next parameter\n",
    "                for value in param_values[depth]:\n",
    "                    current_params[param_names[depth]] = value\n",
    "                    nested_loop_search(depth + 1, current_params)\n",
    "        \n",
    "        nested_loop_search(0, {})\n",
    "        return best_model, best_params, best_score\n",
    "\n",
    "trainer = ModelTrainer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1762319301274,
     "user": {
      "displayName": "Neal Wang",
      "userId": "00218255738253830775"
     },
     "user_tz": 480
    },
    "id": "ew0M0oAn6Ur9"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    roc_curve,\n",
    "    auc\n",
    ")\n",
    "from sklearn.utils import resample\n",
    "\n",
    "\n",
    "def calculate_bootstrap_f1_confidence_interval(y_true, y_pred, n_iterations=1000, average='weighted'):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    f1_scores = []\n",
    "\n",
    "    # Use while loop instead of for loop\n",
    "iteration_count = 0\n",
    "while iteration_count < n_iterations:\n",
    "        indices = resample(np.arange(len(y_true)))\n",
    "        if len(np.unique(y_true[indices])) < 2:\n",
    "            continue\n",
    "        f1 = f1_score(y_true[indices], y_pred[indices], average=average)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    f1_mean = np.mean(f1_scores)\n",
    "    ci_lower = np.percentile(f1_scores, 2.5)\n",
    "    ci_upper = np.percentile(f1_scores, 97.5)\n",
    "    return f1_mean, ci_lower, ci_upper\n",
    "\n",
    "\n",
    "def calculate_bootstrap_auc_confidence_interval(y_true, y_scores, n_iterations=1000):\n",
    "    y_true = np.array(y_true)\n",
    "    y_scores = np.array(y_scores)\n",
    "    auc_scores = []\n",
    "\n",
    "    # Use while loop instead of for loop\n",
    "iteration_count = 0\n",
    "while iteration_count < n_iterations:\n",
    "        indices = resample(np.arange(len(y_true)))\n",
    "        if len(np.unique(y_true[indices])) < 2:\n",
    "            continue\n",
    "        fpr, tpr, _ = roc_curve(y_true[indices], y_scores[indices])\n",
    "        auc_score = auc(fpr, tpr)\n",
    "        auc_scores.append(auc_score)\n",
    "    iteration_count += 1\n",
    "\n",
    "    ci_lower = np.percentile(auc_scores, 2.5)\n",
    "    ci_upper = np.percentile(auc_scores, 97.5)\n",
    "    return np.mean(auc_scores), ci_lower, ci_upper\n",
    "\n",
    "\n",
    "def visualize_confusion_matrix(y_true, y_pred, labels=['No Issue', 'Issue']):\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_roc_curve(y_true, y_scores, label_prefix=\"Model\"):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, label=f'{label_prefix} AUC = {roc_auc:.2f}')\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curve - {label_prefix}')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "    return roc_auc\n",
    "\n",
    "\n",
    "def evaluate_classical_ml_model(model, X_test, y_test, model_name=\"Model\", use_proba=False):\n",
    "    print(f\"\\n--- Evaluation Report: {model_name} ---\\n\")\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    f1_mean, f1_ci_low, f1_ci_high = calculate_bootstrap_f1_confidence_interval(y_test, y_pred)\n",
    "    print(f\"Weighted F1 Score: {f1_mean:.4f}\")\n",
    "    print(f\"95% CI for F1 Score: [{f1_ci_low:.4f}, {f1_ci_high:.4f}]\")\n",
    "\n",
    "    visualize_confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # ROC + AUC\n",
    "    if use_proba:\n",
    "        y_scores = model.predict_proba(X_test)[:, 1]\n",
    "    else:\n",
    "        y_scores = model.decision_function(X_test)\n",
    "\n",
    "    auc_score, auc_ci_low, auc_ci_high = calculate_bootstrap_auc_confidence_interval(y_test, y_scores)\n",
    "    print(f\"AUC Score: {auc_score:.4f}\")\n",
    "    print(f\"95% CI for AUC: [{auc_ci_low:.4f}, {auc_ci_high:.4f}]\")\n",
    "\n",
    "    visualize_roc_curve(y_test, y_scores, label_prefix=model_name)\n",
    "\n",
    "\n",
    "# --- Usage Example ---\n",
    "# For SVM with decision_function:\n",
    "#evaluate_model(SVM, X_test_1, y_test_1, model_name=\"SVM\", use_proba=False)\n",
    "\n",
    "# For models like RandomForest or LogisticRegression with predict_proba:\n",
    "# evaluate_model(rf_model, X_test, y_test, model_name=\"Random Forest\", use_proba=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 Linear Classification: Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Logistic Regression Hyperparameter Tuning\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "lr_param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'solver': ['liblinear', 'lbfgs'],\n",
    "    'penalty': ['l2'],\n",
    "    'class_weight': ['balanced', None]\n",
    "}\n",
    "\n",
    "start_time = time.time()\n",
    "best_model_lr, best_params_lr, best_f1_score = trainer.grid_search(\n",
    "    lr_param_grid,\n",
    "    LogisticRegression,\n",
    "    train_features_classical,\n",
    "    train_labels_classical,\n",
    "    val_features_classical,\n",
    "    val_labels_classical,\n",
    "    random_state=42,\n",
    "    max_iter=MAX_ITER\n",
    ")\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"\\nBest Hyperparameters: {best_params_lr}\")\n",
    "print(f\"Best Validation F1-Score: {best_f1_score:.4f}\")\n",
    "print(f\"Total Parameter Tuning Time: {end_time - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Train final Logistic Regression model\n",
    "logistic_regression_model = LogisticRegression(\n",
    "    C=10, solver='lbfgs', penalty='l2', \n",
    "    class_weight=None, random_state=42, max_iter=MAX_ITER\n",
    ")\n",
    "logistic_regression_model.fit(train_features_classical, train_labels_classical)\n",
    "\n",
    "# Evaluate\n",
    "evaluate_classical_ml_model(\n",
    "    logistic_regression_model, \n",
    "    test_features_classical, \n",
    "    test_labels_classical, \n",
    "    model_name=\"Logistic Regression\", \n",
    "    use_proba=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Logistic Regression Feature Importance\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "coefficients = logistic_regression_model.coef_[0]\n",
    "\n",
    "importance_dataframe = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Coefficient': coefficients\n",
    "})\n",
    "\n",
    "importance_dataframe['Importance (Abs)'] = importance_dataframe['Coefficient'].abs()\n",
    "importance_dataframe['Scaled Importance'] = (\n",
    "    importance_dataframe['Importance (Abs)'] / importance_dataframe['Importance (Abs)'].max()\n",
    ") * 100\n",
    "\n",
    "top_20_features = importance_dataframe.nlargest(20, 'Importance (Abs)')\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.barplot(\n",
    "    x='Scaled Importance',\n",
    "    y='Feature',\n",
    "    data=top_20_features.sort_values('Scaled Importance'),\n",
    "    palette=\"crest\"\n",
    ")\n",
    "plt.title(\"Top 20 Important Features (Logistic Regression)\", fontsize=14, fontweight='bold')\n",
    "plt.xlabel(\"Relative Importance (%)\", fontsize=12)\n",
    "plt.ylabel(\"TF-IDF Feature\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"top_features_lr.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(top_20_features[['Feature', 'Coefficient', 'Scaled Importance']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 Ensemble Method: Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Random Forest Hyperparameter Tuning\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "start_time = time.time()\n",
    "best_model_rf, best_params_rf, best_f1_score_rf = trainer.grid_search(\n",
    "    rf_param_grid,\n",
    "    RandomForestClassifier,\n",
    "    train_features_classical,\n",
    "    train_labels_classical,\n",
    "    val_features_classical,\n",
    "    val_labels_classical,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"\\nBest Hyperparameters: {best_params_rf}\")\n",
    "print(f\"Best Validation F1-Score: {best_f1_score_rf:.4f}\")\n",
    "print(f\"Total Parameter Tuning Time: {end_time - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 Ensemble Method: Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Random Forest Hyperparameter Tuning\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "start_time = time.time()\n",
    "best_model_rf, best_params_rf, best_f1_score_rf = trainer.grid_search(\n",
    "    rf_param_grid,\n",
    "    RandomForestClassifier,\n",
    "    train_features_classical,\n",
    "    train_labels_classical,\n",
    "    val_features_classical,\n",
    "    val_labels_classical,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"\\nBest Hyperparameters: {best_params_rf}\")\n",
    "print(f\"Best Validation F1-Score: {best_f1_score_rf:.4f}\")\n",
    "print(f\"Total Parameter Tuning Time: {end_time - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Train final Random Forest model\n",
    "random_forest_model = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=None,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "random_forest_model.fit(train_features_classical, train_labels_classical)\n",
    "\n",
    "# Evaluate\n",
    "evaluate_classical_ml_model(\n",
    "    random_forest_model,\n",
    "    test_features_classical,\n",
    "    test_labels_classical,\n",
    "    model_name=\"Random Forest\",\n",
    "    use_proba=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Random Forest Feature Importance\n",
    "if random_forest_model is not None:\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "    feature_importance = random_forest_model.feature_importances_\n",
    "    feature_importance = (feature_importance / feature_importance.max()) * 100\n",
    "    \n",
    "    important_features = sorted(\n",
    "        zip(feature_importance, feature_names),\n",
    "        key=lambda x: x[0],\n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    top_n = 20\n",
    "    top_features = important_features[:top_n]\n",
    "    importances, names = zip(*top_features)\n",
    "    \n",
    "    importances = importances[::-1]\n",
    "    names = names[::-1]\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    sns.barplot(x=list(importances), y=list(names), palette=\"viridis\")\n",
    "    \n",
    "    plt.title(\"Top 20 Important Features (Random Forest)\", fontsize=14, fontweight='bold')\n",
    "    plt.xlabel(\"Relative Importance (%)\", fontsize=12)\n",
    "    plt.ylabel(\"TF-IDF Feature\", fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"top_features_rf.png\", dpi=300)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3 Gradient Boosting: LightGBM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# LightGBM Hyperparameter Tuning\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "lgbm_param_grid = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'max_depth': [-1, 10],\n",
    "    'num_leaves': [31, 50],\n",
    "    'min_child_samples': [10, 20],\n",
    "    'class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "start_time = time.time()\n",
    "best_model_lgbm, best_params_lgbm, best_f1_score_lgbm = trainer.grid_search(\n",
    "    lgbm_param_grid,\n",
    "    LGBMClassifier,\n",
    "    train_features_classical,\n",
    "    train_labels_classical,\n",
    "    val_features_classical,\n",
    "    val_labels_classical,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    importance_type=\"gain\"\n",
    ")\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"\\nBest Hyperparameters: {best_params_lgbm}\")\n",
    "print(f\"Best Validation F1-Score: {best_f1_score_lgbm:.4f}\")\n",
    "print(f\"Total Parameter Tuning Time: {end_time - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Train final LightGBM model\n",
    "lightgbm_model = LGBMClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=-1,\n",
    "    num_leaves=50,\n",
    "    min_child_samples=10,\n",
    "    class_weight=None,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    importance_type=\"gain\"\n",
    ")\n",
    "lightgbm_model.fit(train_features_classical, train_labels_classical)\n",
    "\n",
    "# Evaluate\n",
    "evaluate_classical_ml_model(\n",
    "    lightgbm_model,\n",
    "    test_features_classical,\n",
    "    test_labels_classical,\n",
    "    model_name=\"LightGBM\",\n",
    "    use_proba=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# LightGBM Feature Importance\n",
    "if lightgbm_model is not None:\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "    feature_importance = lightgbm_model.feature_importances_\n",
    "    feature_importance = (feature_importance / feature_importance.max()) * 100\n",
    "    \n",
    "    important_features = sorted(\n",
    "        zip(feature_importance, feature_names),\n",
    "        key=lambda x: x[0],\n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    top_n = 20\n",
    "    top_features = important_features[:top_n]\n",
    "    importances, names = zip(*top_features)\n",
    "    \n",
    "    importances = importances[::-1]\n",
    "    names = names[::-1]\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    sns.barplot(x=list(importances), y=list(names), palette=\"viridis\")\n",
    "    \n",
    "    plt.title(\"Top 20 Important Features (LightGBM)\", fontsize=14, fontweight='bold')\n",
    "    plt.xlabel(\"Relative Importance (%)\", fontsize=12)\n",
    "    plt.ylabel(\"TF-IDF Feature\", fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"top_features_lgbm.png\", dpi=300)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Model Performance Summary\n",
    "models_performance = {\n",
    "    'Logistic Regression': {\n",
    "        'best_params': best_params_lr if 'best_params_lr' in globals() else None,\n",
    "        'best_f1': best_f1_score if 'best_f1_score' in globals() else None\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'best_params': best_params_rf if 'best_params_rf' in globals() else None,\n",
    "        'best_f1': best_f1_score_rf if 'best_f1_score_rf' in globals() else None\n",
    "    },\n",
    "    'LightGBM': {\n",
    "        'best_params': best_params_lgbm if 'best_params_lgbm' in globals() else None,\n",
    "        'best_f1': best_f1_score_lgbm if 'best_f1_score_lgbm' in globals() else None\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison_data = []\n",
    "for model_name, perf in models_performance.items():\n",
    "    if perf['best_f1'] is not None:\n",
    "        comparison_data.append({\n",
    "            'Model': model_name,\n",
    "            'Best F1 Score': perf['best_f1'],\n",
    "            'Best Parameters': str(perf['best_params'])\n",
    "        })\n",
    "\n",
    "if comparison_data:\n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    comparison_df = comparison_df.sort_values('Best F1 Score', ascending=False)\n",
    "    print(\"\\nModel Performance Comparison:\")\n",
    "    print(comparison_df.to_string(index=False))\n",
    "    \n",
    "    # Visualize comparison\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    comparison_df.plot(x='Model', y='Best F1 Score', kind='barh', ax=ax, legend=False)\n",
    "    ax.set_xlabel('Best F1 Score')\n",
    "    ax.set_title('Model Performance Comparison (Validation F1 Score)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "Uc2JWqJKBRwH",
    "PewSwsEtYJmn",
    "OOFKjTzEZgwT",
    "Xxf9Kwwjgt1Q",
    "qzNU0z_-i8H_",
    "kqpnRHGhjAoN"
   ],
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
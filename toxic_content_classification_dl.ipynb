{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Toxic Content with Neural Network Architectures\n",
    "\n",
    "This notebook implements deep learning models: GRU and ALBERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Third-party imports - Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Third-party imports - Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Third-party imports - Machine Learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, f1_score\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Third-party imports - Deep Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# Third-party imports - Transformers\n",
    "from transformers import (\n",
    "    AlbertTokenizer, AlbertForSequenceClassification\n",
    ")\n",
    "\n",
    "# Third-party imports - Utilities\n",
    "from tqdm import tqdm\n",
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Google Colab setup\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive/', force_remount=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration and Constants\n",
    "RANDOM_SEED = 1234\n",
    "TEST_SIZE = 0.2\n",
    "VAL_SIZE = 0.25\n",
    "DATA_PATH = '/content/drive/MyDrive/hatespeech/hatexplain_detailed.csv'\n",
    "MAX_SEQUENCE_LENGTH_ALBERT = 200\n",
    "MAX_SEQUENCE_LENGTH_GRU = 200\n",
    "VOCAB_SIZE_GRU = 10000\n",
    "BATCH_SIZE_ALBERT = 4\n",
    "BATCH_SIZE_GRU = 16\n",
    "N_RANDOM_SEARCH_ITERATIONS_ALBERT = 10\n",
    "N_RANDOM_SEARCH_ITERATIONS_GRU = 10\n",
    "N_BOOTSTRAP_ITERATIONS = 1000\n",
    "ALBERT_MODEL_NAME = 'albert-base-v2'\n",
    "MODEL_SAVE_PATH_ALBERT = '/content/drive/MyDrive/best_albert_binary_model_v1.pth'\n",
    "MODEL_SAVE_PATH_GRU = 'GRU_Binary.pth'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading and Initial Processing\n",
    "def load_and_prepare_data(file_path, random_seed=RANDOM_SEED):\n",
    "    \"\"\"Load data and perform initial transformations.\"\"\"\n",
    "    raw_data = pd.read_csv(file_path, index_col=0)\n",
    "    processed_data = raw_data.reset_index()\n",
    "    processed_data = processed_data[['text', 'text_type']]\n",
    "    \n",
    "    # Convert labels using map() instead of apply()\n",
    "    processed_data['text_type'] = processed_data['text_type'].map(\n",
    "        lambda x: 0 if x == 'normal' else 1\n",
    "    )\n",
    "    \n",
    "    # Shuffle using sample() instead of shuffle()\n",
    "    processed_data = processed_data.sample(\n",
    "        frac=1, random_state=random_seed\n",
    "    ).reset_index(drop=True)\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "dataframe_main = load_and_prepare_data(DATA_PATH)\n",
    "print(f\"Dataset shape: {dataframe_main.shape}\")\n",
    "print(f\"\\nDataset info:\")\n",
    "print(dataframe_main.describe().T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Splitting Function\n",
    "def split_data_for_dl(features, labels, test_size=TEST_SIZE, val_size=VAL_SIZE, random_seed=RANDOM_SEED):\n",
    "    \"\"\"Split data into train, validation, and test sets.\"\"\"\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        features, labels, test_size=test_size, random_state=random_seed\n",
    "    )\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp, y_temp, test_size=val_size, random_state=random_seed\n",
    "    )\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Preprocessing for Deep Learning Models\n",
    "class DLTextPreprocessor:\n",
    "    \"\"\"Handles text preprocessing for deep learning models.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def clean_text_albert(text):\n",
    "        \"\"\"Clean text for ALBERT (minimal preprocessing).\"\"\"\n",
    "        text = str(text).lower()\n",
    "        text = re.sub(pattern=r\"http\\S+\", repl=\"<URL>\", string=text)\n",
    "        text = re.sub(pattern=r\"@\\w+\", repl=\"<USER>\", string=text)\n",
    "        text = re.sub(pattern=r\"#\\w+\", repl=\"<HASHTAG>\", string=text)\n",
    "        text = re.sub(pattern=r\"[^\\w\\s]\", repl=\"\", string=text)\n",
    "        text = re.sub(pattern=r\"\\s+\", repl=\" \", string=text)\n",
    "        return text.strip()\n",
    "    \n",
    "    @staticmethod\n",
    "    def clean_text_gru(text):\n",
    "        \"\"\"Clean text for GRU (minimal preprocessing).\"\"\"\n",
    "        text = str(text).lower()\n",
    "        text = re.sub(pattern=r\"http\\S+\", repl=\"<URL>\", string=text)\n",
    "        text = re.sub(pattern=r\"@\\w+\", repl=\"<USER>\", string=text)\n",
    "        text = re.sub(pattern=r\"#\\w+\", repl=\"<HASHTAG>\", string=text)\n",
    "        text = re.sub(pattern=r\"[^\\w\\s]\", repl=\"\", string=text)\n",
    "        text = re.sub(pattern=r\"\\s+\", repl=\" \", string=text)\n",
    "        return text.strip()\n",
    "\n",
    "dl_preprocessor = DLTextPreprocessor()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Neural Network Model Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Recurrent Neural Network: GRU Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for GRU\n",
    "dataframe_gru = dataframe_main.copy()\n",
    "dataframe_gru['text'] = list(map(dl_preprocessor.clean_text_gru, dataframe_gru['text']))\n",
    "\n",
    "# Split data\n",
    "features_gru = list(dataframe_gru['text'])\n",
    "labels_gru = list(dataframe_gru['text_type'])\n",
    "\n",
    "train_texts_gru, val_texts_gru, test_texts_gru, \\\n",
    "train_labels_gru, val_labels_gru, test_labels_gru = split_data_for_dl(\n",
    "    features_gru, labels_gru\n",
    ")\n",
    "\n",
    "print(f\"GRU - Training: {len(train_texts_gru)}, Validation: {len(val_texts_gru)}, Test: {len(test_texts_gru)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Vocabulary and Convert to Sequences\n",
    "def build_vocabulary(texts, vocab_size=VOCAB_SIZE_GRU):\n",
    "    \"\"\"Build vocabulary from training texts.\"\"\"\n",
    "    all_words = ' '.join(texts).split()\n",
    "    most_common_words = [word for word, _ in Counter(all_words).most_common(vocab_size - 1)]\n",
    "    word_to_index = {word: idx + 1 for idx, word in enumerate(most_common_words)}\n",
    "    return word_to_index\n",
    "\n",
    "def convert_text_to_sequence_indices(text, word_to_index, max_len=MAX_SEQUENCE_LENGTH_GRU):\n",
    "    \"\"\"Convert text to sequence of indices.\"\"\"\n",
    "    tokens = text.split()\n",
    "    sequence = [word_to_index.get(word, 0) for word in tokens]\n",
    "    return sequence[:max_len]\n",
    "\n",
    "# Build vocabulary from training data only\n",
    "word_to_index_gru = build_vocabulary(train_texts_gru, VOCAB_SIZE_GRU)\n",
    "\n",
    "# Convert texts to sequences using map()\n",
    "train_sequences_gru = list(map(lambda t: convert_text_to_sequence_indices(t, word_to_index_gru), train_texts_gru))\n",
    "val_sequences_gru = list(map(lambda t: convert_text_to_sequence_indices(t, word_to_index_gru), val_texts_gru))\n",
    "test_sequences_gru = list(map(lambda t: convert_text_to_sequence_indices(t, word_to_index_gru), test_texts_gru))\n",
    "\n",
    "# Pad sequences\n",
    "train_sequences_gru = nn.utils.rnn.pad_sequence(\n",
    "    [torch.tensor(seq) for seq in train_sequences_gru], \n",
    "    batch_first=True, \n",
    "    padding_value=0\n",
    ")\n",
    "val_sequences_gru = nn.utils.rnn.pad_sequence(\n",
    "    [torch.tensor(seq) for seq in val_sequences_gru], \n",
    "    batch_first=True, \n",
    "    padding_value=0\n",
    ")\n",
    "test_sequences_gru = nn.utils.rnn.pad_sequence(\n",
    "    [torch.tensor(seq) for seq in test_sequences_gru], \n",
    "    batch_first=True, \n",
    "    padding_value=0\n",
    ")\n",
    "\n",
    "# Convert labels to tensors\n",
    "train_labels_gru = torch.tensor(train_labels_gru, dtype=torch.long)\n",
    "val_labels_gru = torch.tensor(val_labels_gru, dtype=torch.long)\n",
    "test_labels_gru = torch.tensor(test_labels_gru, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU Dataset Class\n",
    "class GRUSentimentDataset(Dataset):\n",
    "    \"\"\"Dataset class for GRU model.\"\"\"\n",
    "    \n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.sequences[idx],\n",
    "            'label': self.labels[idx]\n",
    "        }\n",
    "\n",
    "# Create datasets\n",
    "train_dataset_gru = GRUSentimentDataset(train_sequences_gru, train_labels_gru)\n",
    "val_dataset_gru = GRUSentimentDataset(val_sequences_gru, val_labels_gru)\n",
    "test_dataset_gru = GRUSentimentDataset(test_sequences_gru, test_labels_gru)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader_gru = DataLoader(train_dataset_gru, batch_size=BATCH_SIZE_GRU, shuffle=True)\n",
    "val_loader_gru = DataLoader(val_dataset_gru, batch_size=BATCH_SIZE_GRU)\n",
    "test_loader_gru = DataLoader(test_dataset_gru, batch_size=BATCH_SIZE_GRU)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU Model Definition\n",
    "class GRUSentimentModel(nn.Module):\n",
    "    \"\"\"GRU-based sentiment classification model.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, dropout_rate=0.3):\n",
    "        super(GRUSentimentModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        embedded = self.embedding(input_ids)\n",
    "        _, hidden = self.gru(embedded)\n",
    "        output = self.fc(self.dropout(hidden[-1]))\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU Training and Evaluation Function\n",
    "def train_and_evaluate_gru_model(hyperparams, train_loader, val_loader, vocab_size, output_dim, device):\n",
    "    \"\"\"Train and evaluate GRU model with given hyperparameters.\"\"\"\n",
    "    embedding_dim = int(hyperparams['embedding_dim'])\n",
    "    hidden_dim = int(hyperparams['hidden_dim'])\n",
    "    lr = hyperparams['lr']\n",
    "\n",
    "    # Initialize model\n",
    "    model = GRUSentimentModel(vocab_size, embedding_dim, hidden_dim, output_dim).to(device)\n",
    "\n",
    "    # Compute class weights\n",
    "    all_train_labels = [label.item() for batch in train_loader for label in batch['label']]\n",
    "    class_weights = compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=np.unique(all_train_labels),\n",
    "        y=all_train_labels\n",
    "    )\n",
    "    class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Training loop using while instead of for\n",
    "    epoch_count = 0\n",
    "    while epoch_count < hyperparams['epochs']:\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        train_loop = tqdm(train_loader, desc=f\"Training Epoch {epoch_count + 1}/{hyperparams['epochs']}\", leave=True, position=0)\n",
    "        \n",
    "        for batch in train_loop:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loop.set_postfix(loss=loss.item())\n",
    "        \n",
    "        print(f\"Epoch {epoch_count + 1}/{hyperparams['epochs']}, Loss: {total_loss / len(train_loader):.4f}\")\n",
    "        epoch_count += 1\n",
    "\n",
    "    # Validation evaluation\n",
    "    model.eval()\n",
    "    val_predictions, val_labels = [], []\n",
    "    val_loop = tqdm(val_loader, desc=\"Validating\", leave=True, position=0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loop:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            outputs = model(input_ids)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            val_predictions.extend(preds.cpu().numpy())\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calculate metrics\n",
    "    f1 = f1_score(val_labels, val_predictions, average='weighted')\n",
    "    accuracy = accuracy_score(val_labels, val_predictions)\n",
    "    print(f\"Validation F-1 Score: {f1:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "    return f1, accuracy, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU Hyperparameter Tuning\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "def tune_gru_hyperparameters_random_search(train_loader, val_loader, vocab_size, output_dim, device, n_iter=N_RANDOM_SEARCH_ITERATIONS_GRU):\n",
    "    \"\"\"Perform random search hyperparameter tuning for GRU.\"\"\"\n",
    "    param_space = {\n",
    "        'embedding_dim': (150, 250),\n",
    "        'hidden_dim': (256, 768),\n",
    "        'lr': (np.log10(1e-4), np.log10(1e-3)),\n",
    "        'epochs': (5, 10)\n",
    "    }\n",
    "\n",
    "    best_f1_score_gru = 0\n",
    "    best_params_gru = None\n",
    "    best_model_gru = None\n",
    "\n",
    "    # Random search using while loop\n",
    "    iteration_count = 0\n",
    "    with tqdm(total=n_iter, desc=\"Random Search Tuning\", leave=True, position=0) as pbar:\n",
    "        while iteration_count < n_iter:\n",
    "            # Sample hyperparameters\n",
    "            hyperparams = {\n",
    "                'embedding_dim': np.random.uniform(*param_space['embedding_dim']),\n",
    "                'hidden_dim': np.random.uniform(*param_space['hidden_dim']),\n",
    "                'lr': 10**np.random.uniform(*param_space['lr']),\n",
    "                'epochs': np.random.randint(*param_space['epochs'])\n",
    "            }\n",
    "            \n",
    "            f1, accuracy, model = train_and_evaluate_gru_model(\n",
    "                hyperparams, train_loader, val_loader, vocab_size, output_dim, device\n",
    "            )\n",
    "            \n",
    "            pbar.set_postfix(f1=f1, accuracy=accuracy)\n",
    "            pbar.update(1)\n",
    "\n",
    "            if f1 > best_f1_score_gru:\n",
    "                best_f1_score_gru = f1\n",
    "                best_params_gru = hyperparams.copy()\n",
    "                best_model_gru = model\n",
    "            \n",
    "            iteration_count += 1\n",
    "\n",
    "    print(f\"Best Validation F1 Score: {best_f1_score_gru:.4f}\")\n",
    "    print(f\"Best Hyperparameters: {best_params_gru}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if best_model_gru is not None:\n",
    "        torch.save(best_model_gru.state_dict(), MODEL_SAVE_PATH_GRU)\n",
    "    \n",
    "    return best_params_gru, best_f1_score_gru, best_model_gru\n",
    "\n",
    "# Run hyperparameter tuning\n",
    "start_time = time.time()\n",
    "best_params_gru, best_f1_score_gru, best_model_gru = tune_gru_hyperparameters_random_search(\n",
    "    train_loader_gru,\n",
    "    val_loader_gru,\n",
    "    vocab_size=VOCAB_SIZE_GRU,\n",
    "    output_dim=2,\n",
    "    device=device,\n",
    "    n_iter=N_RANDOM_SEARCH_ITERATIONS_GRU\n",
    ")\n",
    "end_time = time.time()\n",
    "print(f\"Total Parameter Tuning Time: {end_time - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final GRU model with best hyperparameters\n",
    "hyperparams_gru = dict(\n",
    "    embedding_dim=156.02979350260784,\n",
    "    hidden_dim=467.03628008471236,\n",
    "    lr=0.000411395570516993,\n",
    "    epochs=5\n",
    ")\n",
    "\n",
    "embedding_dim = int(hyperparams_gru['embedding_dim'])\n",
    "hidden_dim = int(hyperparams_gru['hidden_dim'])\n",
    "lr = hyperparams_gru['lr']\n",
    "\n",
    "# Initialize model\n",
    "gru_model = GRUSentimentModel(VOCAB_SIZE_GRU, embedding_dim, hidden_dim, 2).to(device)\n",
    "\n",
    "# Compute class weights\n",
    "all_train_labels = [label.item() for batch in train_loader_gru for label in batch['label']]\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(all_train_labels),\n",
    "    y=all_train_labels\n",
    ")\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "optimizer = torch.optim.Adam(gru_model.parameters(), lr=lr)\n",
    "\n",
    "# Training loop\n",
    "epoch_count = 0\n",
    "while epoch_count < hyperparams_gru['epochs']:\n",
    "    gru_model.train()\n",
    "    total_loss = 0\n",
    "    train_loop = tqdm(train_loader_gru, desc=f\"Training Epoch {epoch_count + 1}/{hyperparams_gru['epochs']}\", leave=True, position=0)\n",
    "    \n",
    "    for batch in train_loop:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = gru_model(input_ids)\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loop.set_postfix(loss=loss.item())\n",
    "    \n",
    "    print(f\"Epoch {epoch_count + 1}/{hyperparams_gru['epochs']}, Loss: {total_loss / len(train_loader_gru):.4f}\")\n",
    "    epoch_count += 1\n",
    "\n",
    "# Save model\n",
    "torch.save(gru_model.state_dict(), MODEL_SAVE_PATH_GRU)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate GRU model\n",
    "gru_model = GRUSentimentModel(VOCAB_SIZE_GRU, embedding_dim, hidden_dim, 2).to(device)\n",
    "gru_model.load_state_dict(torch.load(MODEL_SAVE_PATH_GRU, map_location=device))\n",
    "\n",
    "evaluate_deep_learning_model(\n",
    "    gru_model, \n",
    "    test_loader_gru, \n",
    "    device, \n",
    "    model_name=\"GRU\", \n",
    "    num_classes=2, \n",
    "    use_attention_mask=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Transformer-Based Model: ALBERT Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for ALBERT\n",
    "dataframe_albert = dataframe_main.copy()\n",
    "dataframe_albert['text'] = list(map(dl_preprocessor.clean_text_albert, dataframe_albert['text']))\n",
    "\n",
    "# Split data\n",
    "features_albert = list(dataframe_albert['text'])\n",
    "labels_albert = list(dataframe_albert['text_type'])\n",
    "\n",
    "train_texts_albert, val_texts_albert, test_texts_albert, \\\n",
    "train_labels_albert, val_labels_albert, test_labels_albert = split_data_for_dl(\n",
    "    features_albert, labels_albert\n",
    ")\n",
    "\n",
    "print(f\"ALBERT - Training: {len(train_texts_albert)}, Validation: {len(val_texts_albert)}, Test: {len(test_texts_albert)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALBERT Dataset Class\n",
    "class AlbertSentimentDataset(Dataset):\n",
    "    \"\"\"Dataset class for ALBERT model.\"\"\"\n",
    "    \n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = list(texts)\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Initialize tokenizer and create datasets\n",
    "albert_tokenizer = AlbertTokenizer.from_pretrained(ALBERT_MODEL_NAME)\n",
    "\n",
    "train_dataset_albert = AlbertSentimentDataset(train_texts_albert, train_labels_albert, albert_tokenizer, MAX_SEQUENCE_LENGTH_ALBERT)\n",
    "val_dataset_albert = AlbertSentimentDataset(val_texts_albert, val_labels_albert, albert_tokenizer, MAX_SEQUENCE_LENGTH_ALBERT)\n",
    "test_dataset_albert = AlbertSentimentDataset(test_texts_albert, test_labels_albert, albert_tokenizer, MAX_SEQUENCE_LENGTH_ALBERT)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader_albert = DataLoader(train_dataset_albert, batch_size=BATCH_SIZE_ALBERT, shuffle=True)\n",
    "val_loader_albert = DataLoader(val_dataset_albert, batch_size=BATCH_SIZE_ALBERT)\n",
    "test_loader_albert = DataLoader(test_dataset_albert, batch_size=BATCH_SIZE_ALBERT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALBERT Hyperparameter Tuning\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "def tune_albert_hyperparameters_random_search(train_texts, train_labels, val_texts, val_labels, tokenizer, n_iter=N_RANDOM_SEARCH_ITERATIONS_ALBERT):\n",
    "    \"\"\"Perform random search hyperparameter tuning for ALBERT.\"\"\"\n",
    "    param_space = {\n",
    "        'lr': (np.log10(1e-5), np.log10(1e-4)),\n",
    "        'epochs': (3, 5),\n",
    "        'dropout': (0.1, 0.5)\n",
    "    }\n",
    "\n",
    "    best_f1_score_albert = 0\n",
    "    best_params_albert = None\n",
    "    best_model_albert = None\n",
    "\n",
    "    # Compute class weights\n",
    "    class_weights = compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=np.unique(train_labels),\n",
    "        y=train_labels\n",
    "    )\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "    # Random search using while loop instead of for loop\n",
    "    iteration_count = 0\n",
    "    with tqdm(total=n_iter, desc=\"Random Search Tuning\", leave=True) as pbar:\n",
    "        while iteration_count < n_iter:\n",
    "            # Randomly sample hyperparameters\n",
    "            hyperparams = {\n",
    "                'lr': 10**np.random.uniform(*param_space['lr']),\n",
    "                'epochs': np.random.randint(*param_space['epochs']),\n",
    "                'dropout': np.random.uniform(*param_space['dropout'])\n",
    "            }\n",
    "\n",
    "            # Initialize model\n",
    "            model = AlbertForSequenceClassification.from_pretrained(\n",
    "                ALBERT_MODEL_NAME,\n",
    "                num_labels=2,\n",
    "                hidden_dropout_prob=hyperparams['dropout']\n",
    "            ).to(device)\n",
    "\n",
    "            optimizer = AdamW(model.parameters(), lr=hyperparams['lr'])\n",
    "            criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "            # Training loop\n",
    "            epoch_count = 0\n",
    "            while epoch_count < hyperparams['epochs']:\n",
    "                model.train()\n",
    "                for batch in train_loader_albert:\n",
    "                    input_ids = batch['input_ids'].to(device)\n",
    "                    attention_mask = batch['attention_mask'].to(device)\n",
    "                    labels = batch['label'].to(device)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                    loss = criterion(outputs.logits, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                epoch_count += 1\n",
    "\n",
    "            # Validation evaluation\n",
    "            model.eval()\n",
    "            val_predictions, val_labels_batch = [], []\n",
    "            with torch.no_grad():\n",
    "                for batch in val_loader_albert:\n",
    "                    input_ids = batch['input_ids'].to(device)\n",
    "                    attention_mask = batch['attention_mask'].to(device)\n",
    "                    labels = batch['label'].to(device)\n",
    "\n",
    "                    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                    preds = torch.argmax(outputs.logits, dim=1)\n",
    "                    val_predictions.extend(preds.cpu().numpy())\n",
    "                    val_labels_batch.extend(labels.cpu().numpy())\n",
    "\n",
    "            # Calculate F1-score\n",
    "            f1 = f1_score(val_labels_batch, val_predictions, average='weighted')\n",
    "            accuracy = accuracy_score(val_labels_batch, val_predictions)\n",
    "\n",
    "            # Update best model\n",
    "            if f1 > best_f1_score_albert:\n",
    "                best_f1_score_albert = f1\n",
    "                best_params_albert = hyperparams.copy()\n",
    "                best_model_albert = model\n",
    "                torch.save(model.state_dict(), \"best_albert_model.pth\")\n",
    "                torch.save(model.state_dict(), MODEL_SAVE_PATH_ALBERT)\n",
    "\n",
    "            pbar.set_postfix(f1=f1, accuracy=accuracy)\n",
    "            pbar.update(1)\n",
    "            iteration_count += 1\n",
    "\n",
    "    print(f\"Best F1 Score: {best_f1_score_albert:.4f}\")\n",
    "    print(f\"Best Hyperparameters: {best_params_albert}\")\n",
    "    return best_params_albert, best_f1_score_albert, best_model_albert\n",
    "\n",
    "# Run hyperparameter tuning\n",
    "start_time = time.time()\n",
    "best_params_albert, best_f1_score_albert, best_model_albert = tune_albert_hyperparameters_random_search(\n",
    "    train_texts=train_texts_albert,\n",
    "    train_labels=train_labels_albert,\n",
    "    val_texts=val_texts_albert,\n",
    "    val_labels=val_labels_albert,\n",
    "    tokenizer=albert_tokenizer,\n",
    "    n_iter=N_RANDOM_SEARCH_ITERATIONS_ALBERT\n",
    ")\n",
    "end_time = time.time()\n",
    "print(f\"Total Parameter Tuning Time: {end_time - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and evaluate ALBERT model\n",
    "albert_model = AlbertForSequenceClassification.from_pretrained(ALBERT_MODEL_NAME, num_labels=2)\n",
    "albert_model.load_state_dict(torch.load(MODEL_SAVE_PATH_ALBERT, map_location=device))\n",
    "albert_model.to(device)\n",
    "\n",
    "evaluate_deep_learning_model(\n",
    "    albert_model, \n",
    "    test_loader_albert, \n",
    "    device, \n",
    "    model_name=\"ALBERT\", \n",
    "    num_classes=2, \n",
    "    use_attention_mask=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Learning Model Evaluation Functions\n",
    "def evaluate_deep_learning_model(model, test_loader, device, model_name=\"DL Model\", num_classes=2, use_attention_mask=True):\n",
    "    \"\"\"Evaluate deep learning model with bootstrap confidence intervals.\"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            if use_attention_mask:\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            else:\n",
    "                outputs = model(input_ids)\n",
    "\n",
    "            logits = outputs.logits if hasattr(outputs, 'logits') else outputs\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_probs = np.array(all_probs)\n",
    "\n",
    "    if num_classes == 2:\n",
    "        positive_probs = all_probs[:, 1]\n",
    "    else:\n",
    "        positive_probs = None\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    print(f\"\\n--- Evaluation Report: {model_name} ---\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"Weighted F1 Score: {f1:.4f}\")\n",
    "\n",
    "    # F1 CI using while loop instead of for loop\n",
    "    f1_scores = []\n",
    "    iteration_count = 0\n",
    "    while iteration_count < N_BOOTSTRAP_ITERATIONS:\n",
    "        idx = resample(np.arange(len(all_labels)))\n",
    "        if len(np.unique(all_labels[idx])) < 2:\n",
    "            continue\n",
    "        f1_bs = f1_score(all_labels[idx], all_preds[idx], average='weighted')\n",
    "        f1_scores.append(f1_bs)\n",
    "        iteration_count += 1\n",
    "    \n",
    "    print(f\"95% CI for F1 Score: [{np.percentile(f1_scores, 2.5):.4f}, {np.percentile(f1_scores, 97.5):.4f}]\")\n",
    "\n",
    "    # Confusion matrix using OOP style\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', ax=ax)\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"Actual\")\n",
    "    ax.set_title(f\"Confusion Matrix - {model_name}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # ROC using OOP style\n",
    "    if num_classes == 2:\n",
    "        fpr, tpr, _ = roc_curve(all_labels, positive_probs)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        auc_scores = []\n",
    "        iteration_count = 0\n",
    "        while iteration_count < N_BOOTSTRAP_ITERATIONS:\n",
    "            idx = resample(np.arange(len(all_labels)))\n",
    "            if len(np.unique(all_labels[idx])) < 2:\n",
    "                continue\n",
    "            fpr_bs, tpr_bs, _ = roc_curve(all_labels[idx], positive_probs[idx])\n",
    "            auc_scores.append(auc(fpr_bs, tpr_bs))\n",
    "            iteration_count += 1\n",
    "        \n",
    "        print(f\"AUC: {roc_auc:.4f}\")\n",
    "        print(f\"95% CI for AUC: [{np.percentile(auc_scores, 2.5):.4f}, {np.percentile(auc_scores, 97.5):.4f}]\")\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        ax.plot(fpr, tpr, label=f\"AUC = {roc_auc:.2f}\")\n",
    "        ax.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "        ax.set_xlabel(\"False Positive Rate\")\n",
    "        ax.set_ylabel(\"True Positive Rate\")\n",
    "        ax.set_title(f\"ROC Curve - {model_name}\")\n",
    "        ax.legend(loc='lower right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical Comparison of Models\n",
    "def run_pairwise_mcnemar_test_for_all_models(y_true, model_preds, method='holm'):\n",
    "    \"\"\"Perform pairwise McNemar's test for all model combinations.\"\"\"\n",
    "    assert all(len(y_true) == len(preds) for preds in model_preds.values()), \"All predictions must match y_true length.\"\n",
    "\n",
    "    results = []\n",
    "    model_names = list(model_preds.keys())\n",
    "\n",
    "    # Run pairwise McNemar tests\n",
    "    for m1, m2 in combinations(model_names, 2):\n",
    "        y_pred_1, y_pred_2 = model_preds[m1], model_preds[m2]\n",
    "\n",
    "        # Create contingency table\n",
    "        both_correct = np.sum((y_pred_1 == y_true) & (y_pred_2 == y_true))\n",
    "        model1_correct = np.sum((y_pred_1 == y_true) & (y_pred_2 != y_true))\n",
    "        model2_correct = np.sum((y_pred_1 != y_true) & (y_pred_2 == y_true))\n",
    "        both_wrong = np.sum((y_pred_1 != y_true) & (y_pred_2 != y_true))\n",
    "\n",
    "        table = [[both_correct, model2_correct],\n",
    "                 [model1_correct, both_wrong]]\n",
    "\n",
    "        result = mcnemar(table, exact=False, correction=True)\n",
    "\n",
    "        # Store results\n",
    "        results.append(dict(\n",
    "            Model_1=m1,\n",
    "            Model_2=m2,\n",
    "            Statistic=result.statistic,\n",
    "            p_value=result.pvalue,\n",
    "            Winner=m1 if model1_correct > model2_correct else (m2 if model2_correct > model1_correct else \"Tie\")\n",
    "        ))\n",
    "\n",
    "    # Correct for multiple testing\n",
    "    raw_pvals = [r[\"p_value\"] for r in results]\n",
    "    _, corrected_pvals, _, _ = multipletests(raw_pvals, method=method)\n",
    "\n",
    "    for i, p_corr in enumerate(corrected_pvals):\n",
    "        results[i][\"Corrected_p_value\"] = p_corr\n",
    "        results[i][\"Significant\"] = \"Yes\" if p_corr < 0.05 else \"No\"\n",
    "\n",
    "    # Print results\n",
    "    df_results = pd.DataFrame(results)\n",
    "    print(\"\\n\ud83d\udcca Pairwise McNemar's Test Results (corrected using '{}'):\\n\".format(method))\n",
    "    print(df_results[[\"Model_1\", \"Model_2\", \"Winner\", \"Statistic\", \"Corrected_p_value\", \"Significant\"]])\n",
    "\n",
    "    return df_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for comparison\n",
    "# Note: This assumes test_labels_albert and test_labels_gru are the same (they should be)\n",
    "# For comparison, we'll use test_labels_albert as ground truth\n",
    "\n",
    "# ALBERT predictions\n",
    "albert_model.eval()\n",
    "test_predictions_albert = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader_albert:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        outputs = albert_model(input_ids, attention_mask=attention_mask)\n",
    "        preds = torch.argmax(outputs.logits, dim=1)\n",
    "        test_predictions_albert.extend(preds.cpu().numpy())\n",
    "\n",
    "# GRU predictions\n",
    "gru_model.eval()\n",
    "test_predictions_gru = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader_gru:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        outputs = gru_model(input_ids)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        test_predictions_gru.extend(preds.cpu().numpy())\n",
    "\n",
    "# Prepare for comparison (use ALBERT test labels as ground truth)\n",
    "test_labels_for_comparison = test_labels_albert\n",
    "\n",
    "model_predictions_dict = {\n",
    "    \"ALBERT\": np.array(test_predictions_albert),\n",
    "    \"GRU\": np.array(test_predictions_gru)\n",
    "}\n",
    "\n",
    "# Run comparison\n",
    "run_pairwise_mcnemar_test_for_all_models(test_labels_for_comparison, model_predictions_dict)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "Uc2JWqJKBRwH",
    "PewSwsEtYJmn",
    "OOFKjTzEZgwT",
    "Xxf9Kwwjgt1Q",
    "qzNU0z_-i8H_",
    "kqpnRHGhjAoN"
   ],
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}